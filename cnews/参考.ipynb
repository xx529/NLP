{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr\n",
    "\n",
    "# 读取词汇表\n",
    "def read_vocab(vocab_dir):\n",
    "    with open(vocab_dir, 'r', encoding='utf-8', errors='ignore') as fp:\n",
    "        words = [_.strip() for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    " \n",
    " \n",
    "# 读取分类目录，固定\n",
    "def read_category():\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "    categories = [x for x in categories]\n",
    "    cat_to_id = dict(zip(categories, range(len(categories)))) \n",
    "    return categories, cat_to_id\n",
    " \n",
    " \n",
    "# 将文件转换为id表示\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    contents, labels = [], []\n",
    "    with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list(content))\n",
    "                    labels.append(label)\n",
    "            except:\n",
    "                pass\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])#将每句话id化\n",
    "        label_id.append(cat_to_id[labels[i]])#每句话对应的类别的id\n",
    "    #\n",
    "    # # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "    #\n",
    "    return x_pad, y_pad\n",
    " \n",
    " \n",
    "# 获取文本的类别及其对应id的字典\n",
    "categories, cat_to_id = read_category()\n",
    "print(categories)\n",
    "# 获取训练文本中所有出现过的字及其所对应的id\n",
    "words, word_to_id = read_vocab('cnews.vocab.txt')\n",
    "#print(words)\n",
    "#print(word_to_id)\n",
    "#print(word_to_id)\n",
    "#获取字数\n",
    "vocab_size = len(words)\n",
    "\n",
    "# 数据加载及分批\n",
    "# 获取训练数据每个字的id和对应标签的one-hot形式\n",
    "x_train, y_train = process_file('cnews.train.txt', word_to_id, cat_to_id, 600)\n",
    "print('x_train=', x_train)\n",
    "x_val, y_val = process_file('cnews.val.txt', word_to_id, cat_to_id, 600)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
